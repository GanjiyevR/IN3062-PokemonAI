{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdd532f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pokedex_number        Name  Type1   Type2  hp  height_m  weight_kg  attack  \\\n",
      "0               1   bulbasaur  Grass  Poison  45       0.7        6.9      49   \n",
      "1               2     ivysaur  Grass  Poison  60       1.0       13.0      62   \n",
      "2               3    venusaur  Grass  Poison  80       2.0      100.0     100   \n",
      "3               4  charmander   Fire     NaN  39       0.6        8.5      52   \n",
      "4               5  charmeleon   Fire     NaN  58       1.1       19.0      64   \n",
      "\n",
      "   defense  speed  sp_attack  sp_defense   classfication  generation  \\\n",
      "0       49     45         65          65    Seed Pok�mon           1   \n",
      "1       63     60         80          80    Seed Pok�mon           1   \n",
      "2      123     80        122         120    Seed Pok�mon           1   \n",
      "3       43     65         60          50  Lizard Pok�mon           1   \n",
      "4       58     80         80          65   Flame Pok�mon           1   \n",
      "\n",
      "   is_legendary  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import save_and_load\n",
    "\n",
    "path = \".\"  #absolute or relative path to the folder containing the file. \n",
    "            #\".\" for current folder\n",
    "\n",
    "#read and print values from the dataset to check if it imported correctly\n",
    "filename_read = os.path.join(path, \"pokemon.csv\")\n",
    "df = pd.read_csv(filename_read)\n",
    "decodeddf = df\n",
    "print(df[0:5])\n",
    "#df = df.sample(frac=1).reset_index(drop=True)\n",
    "#print(df[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92308d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Grass\n",
      "1      Grass\n",
      "2      Grass\n",
      "3       Fire\n",
      "4       Fire\n",
      "5       Fire\n",
      "6      Water\n",
      "7      Water\n",
      "8      Water\n",
      "9        Bug\n",
      "10       Bug\n",
      "11       Bug\n",
      "12       Bug\n",
      "13       Bug\n",
      "14       Bug\n",
      "15    Flying\n",
      "16    Flying\n",
      "17    Flying\n",
      "18    Normal\n",
      "19    Normal\n",
      "Name: Type1, dtype: object\n",
      "0      9\n",
      "1      9\n",
      "2      9\n",
      "3      6\n",
      "4      6\n",
      "5      6\n",
      "6     17\n",
      "7     17\n",
      "8     17\n",
      "9      0\n",
      "10     0\n",
      "11     0\n",
      "12     0\n",
      "13     0\n",
      "14     0\n",
      "15     7\n",
      "16     7\n",
      "17     7\n",
      "18    12\n",
      "19    12\n",
      "Name: Type1, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "#taken from tutorial 3\n",
    "#df = df.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "#encode target names to integers\n",
    "#https://stackoverflow.com/questions/49037286/pandas-string-values-encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "print(df['Type1'][0:20])\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Type1'] = le.fit_transform(df['Type1'])\n",
    "\n",
    "testingdf = df[0:721]\n",
    "holdoutdf = df[722:890]\n",
    "print(testingdf['Type1'][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a73b290c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(721, 8)\n",
      "(721,)\n",
      "(721, 18)\n"
     ]
    }
   ],
   "source": [
    "#collect the columns names for non-target features\n",
    "result = []\n",
    "for x in testingdf.columns:\n",
    "    if (x == 'attack') or (x == 'defense') or (x == 'speed') or (x == 'sp_defense') or (x == 'sp_attack') or (x == 'weight_kg') or (x == 'height_m') or (x == 'hp'):\n",
    "        result.append(x)\n",
    "\n",
    "#get data (often called X) and target (often calle y) and display its shape\n",
    "X = testingdf[result].values\n",
    "yalt = testingdf['Type1'].values\n",
    "print(X.shape)\n",
    "print(yalt.shape)\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "y = keras.utils.to_categorical(testingdf['Type1'].to_numpy())\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24cb03c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nX_train, X_test, y_train, y_test = train_test_split(X, yalt, test_size=0.15, random_state=4)\\n\\n# Create a Perceptron, with its training parameters\\nppn = Perceptron(max_iter=100,tol=0.001,eta0=1)\\n\\n# Train the model\\nppn.fit(X_train,y_train)\\n\\n# Make predication\\ny_pred = ppn.predict(X_test)\\n\\n# Evaluate accuracy\\nprint('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, yalt, test_size=0.15, random_state=4)\n",
    "\n",
    "# Create a Perceptron, with its training parameters\n",
    "ppn = Perceptron(max_iter=100,tol=0.001,eta0=1)\n",
    "\n",
    "# Train the model\n",
    "ppn.fit(X_train,y_train)\n",
    "\n",
    "# Make predication\n",
    "y_pred = ppn.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe62c17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.25517241379310346\n",
      "Accuracy score: 0.2708333333333333\n",
      "Accuracy score: 0.3611111111111111\n",
      "Accuracy score: 0.4097222222222222\n",
      "Accuracy score: 0.3958333333333333\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn import metrics\n",
    "\n",
    "# make a sequential model and train it using KFold splits\n",
    "# has 0 hidden layers\n",
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "kf = KFold(5)\n",
    "\n",
    "for train, test in kf.split(X):\n",
    "    X_train = X[train]\n",
    "    y_train = y[train]\n",
    "    X_test = X[test]\n",
    "    y_test = y[test]\n",
    "\n",
    "    model.fit(X_train,y_train,verbose=0,epochs=128)\n",
    "    pred = model.predict(X_test)\n",
    "    pred = np.argmax(pred,axis=1)\n",
    "    y_compare = np.argmax(y_test,axis=1) \n",
    "    score = metrics.accuracy_score(y_compare, pred)\n",
    "    print(\"Accuracy score: {}\".format(score))\n",
    "\n",
    "# save the model\n",
    "#save_and_load.save_model(model, path, \"seqKFold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fb8a368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# make a prediction using all the data\\npred = model.predict(X) \\n#show the shape if the inputs\\nprint(pred.shape)\\nprint(y.shape)\\npred = np.argmax(pred,axis=1)\\ny_compare = np.argmax(y,axis=1) \\n#print the accuracy of the model\\nscore = metrics.accuracy_score(y_compare, pred)\\nprint(\"Accuracy score: {}\".format(score))\\n\\n#plot a confusion matrix for the predicitons\\nfrom sklearn.metrics import ConfusionMatrixDisplay\\nfrom sklearn.metrics import confusion_matrix\\nimport matplotlib.pyplot as plt\\n\\ncm = confusion_matrix(y_compare, pred)\\n\\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\\ndisp.plot()\\nplt.ylabel(\\'True label\\')\\nplt.xlabel(\\'Predicted label\\')\\nplt.show()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# make a prediction using all the data\n",
    "pred = model.predict(X) \n",
    "#show the shape if the inputs\n",
    "print(pred.shape)\n",
    "print(y.shape)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_compare = np.argmax(y,axis=1) \n",
    "#print the accuracy of the model\n",
    "score = metrics.accuracy_score(y_compare, pred)\n",
    "print(\"Accuracy score: {}\".format(score))\n",
    "\"\"\"\n",
    "#plot a confusion matrix for the predicitons\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "cm = confusion_matrix(y_compare, pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "865fe961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#list the encoded labels and string labels (temporary solution until we find out how to decode labels)\\nprint(testingdf['Type1'].value_counts())\\nprint(decodeddf['Type1'][0:721].value_counts())\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#list the encoded labels and string labels (temporary solution until we find out how to decode labels)\n",
    "print(testingdf['Type1'].value_counts())\n",
    "print(decodeddf['Type1'][0:721].value_counts())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e46870f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 8)\n",
      "(168,)\n",
      "(168, 18)\n",
      "Accuracy score: 0.10714285714285714\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c5d52fd041a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#plot a confusion matrix for the predicitons\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_compare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mdisp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "#collect the columns names for non-target features\n",
    "result = []\n",
    "for x in holdoutdf.columns:\n",
    "    if (x == 'attack') or (x == 'defense') or (x == 'speed') or (x == 'sp_defense') or (x == 'sp_attack') or (x == 'weight_kg') or (x == 'height_m') or (x == 'hp'):\n",
    "        result.append(x)\n",
    "\n",
    "#get data (often called X) and target (often calle y) and display its shape\n",
    "X = holdoutdf[result].values\n",
    "y = holdoutdf['Type1'].values\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "y = keras.utils.to_categorical(holdoutdf['Type1'].to_numpy())\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "model = save_and_load.model_loader(path, \"seqKFold\")\n",
    "\n",
    "pred = model.predict(X)\n",
    "\n",
    "pred = np.argmax(pred,axis=1)\n",
    "\n",
    "y_compare = np.argmax(y,axis=1) \n",
    "score = metrics.accuracy_score(y_compare, pred)\n",
    "\n",
    "# print the accuracy of the model\n",
    "print(\"Accuracy score: {}\".format(score))\n",
    "\n",
    "#plot a confusion matrix for the predicitons\n",
    "cm = confusion_matrix(y_compare, pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "#list the encoded labels and string labels (temporary solution until we find out how to decode labels)\n",
    "print(holdoutdf['Type1'].value_counts())\n",
    "print(decodeddf['Type1'][722:890].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
